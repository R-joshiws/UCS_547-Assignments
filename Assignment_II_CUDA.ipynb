{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1rgm2IA1tRL"
      },
      "source": [
        "# Assignment 2 - CUDA Programming\n",
        "\n",
        "**Name:** [Your Name Here]  \n",
        "**Roll No:** [Your Roll No]  \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQwS1oTk1tRM"
      },
      "source": [
        "## Q1. Identify !, %, and %% in Colab\n",
        "\n",
        "These are special commands in Colab notebooks:\n",
        "- `!` = run shell/terminal commands\n",
        "- `%` = magic command for single line\n",
        "- `%%` = magic command for entire cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flGtK1ZD1tRN"
      },
      "outputs": [],
      "source": [
        "# Example 1: Shell commands with !\n",
        "!pwd\n",
        "!ls\n",
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gpn5J65p1tRN"
      },
      "outputs": [],
      "source": [
        "# Example 2: Line magic with %\n",
        "%time sum([i for i in range(100000)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1LbtSoH1tRO"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Example 3: Cell magic with %%\n",
        "# this times the entire cell execution\n",
        "result = 0\n",
        "for i in range(1000000):\n",
        "    result += i\n",
        "print(f\"Sum = {result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iaClYHm1tRO"
      },
      "source": [
        "## Q2. nvidia-smi Commands\n",
        "\n",
        "nvidia-smi shows GPU information. Here are some useful commands:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pec_gO8e1tRO"
      },
      "outputs": [],
      "source": [
        "# basic command\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1M1WxMuf1tRP"
      },
      "outputs": [],
      "source": [
        "# list GPUs\n",
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iL53hMVG1tRP"
      },
      "outputs": [],
      "source": [
        "# specific info in CSV format\n",
        "!nvidia-smi --query-gpu=name,memory.total,memory.free,memory.used --format=csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQEhc9Sc1tRQ"
      },
      "outputs": [],
      "source": [
        "# temperature and power usage\n",
        "!nvidia-smi --query-gpu=temperature.gpu,power.draw --format=csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkGzwpK_1tRQ"
      },
      "source": [
        "Other useful options:\n",
        "- `nvidia-smi -l 2` (updates every 2 seconds)\n",
        "- `nvidia-smi pmon` (process monitoring)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3nI73X31tRQ"
      },
      "source": [
        "## Q3. Debug Common CUDA Errors\n",
        "\n",
        "Three main errors:\n",
        "1. Zero output (forgot cudaDeviceSynchronize)\n",
        "2. Incorrect indexing (array out of bounds)\n",
        "3. PTX errors (compilation/runtime issues)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDZqQbZZ1tRQ"
      },
      "outputs": [],
      "source": [
        "%%writefile debug.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// check for cuda errors\n",
        "#define CHECK(call) \\\n",
        "{ \\\n",
        "    cudaError_t err = call; \\\n",
        "    if(err != cudaSuccess) { \\\n",
        "        printf(\"Error: %s\\n\", cudaGetErrorString(err)); \\\n",
        "        exit(1); \\\n",
        "    } \\\n",
        "}\n",
        "\n",
        "// Error 1: no sync - output might not show\n",
        "__global__ void test1() {\n",
        "    printf(\"Hello from GPU\\n\");\n",
        "}\n",
        "\n",
        "// Error 2: bad indexing\n",
        "__global__ void bad_kernel(int *arr, int n) {\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    arr[i] = i;  // might be out of bounds!\n",
        "}\n",
        "\n",
        "// Fixed version\n",
        "__global__ void good_kernel(int *arr, int n) {\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if(i < n) {  // check bounds\n",
        "        arr[i] = i;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Test 1 - forgot to sync\n",
        "    printf(\"Test 1 - no sync:\\n\");\n",
        "    test1<<<1,1>>>();\n",
        "    // missing: cudaDeviceSynchronize();\n",
        "\n",
        "    printf(\"\\nTest 2 - with sync:\\n\");\n",
        "    test1<<<1,1>>>();\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Test 2 - array bounds\n",
        "    int *d_arr;\n",
        "    int n = 10;\n",
        "    CHECK(cudaMalloc(&d_arr, n*sizeof(int)));\n",
        "\n",
        "    good_kernel<<<1,20>>>(d_arr, n);  // 20 threads but only 10 elements\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    CHECK(cudaGetLastError());  // check for errors\n",
        "\n",
        "    printf(\"\\nAll tests passed!\\n\");\n",
        "\n",
        "    cudaFree(d_arr);\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxA6Ljo61tRR"
      },
      "outputs": [],
      "source": [
        "!nvcc debug.cu -o debug\n",
        "!./debug"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg2ajvq-1tRR"
      },
      "source": [
        "**Common fixes:**\n",
        "- Always add `cudaDeviceSynchronize()` after kernel calls\n",
        "- Check array bounds with `if(idx < n)`\n",
        "- Use `cudaGetLastError()` to catch errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3W9LmO0A1tRR"
      },
      "source": [
        "## Q4. GPU Kernel with Thread Indexing\n",
        "\n",
        "Task: Launch kernel with 1 block, 8 threads. Each thread prints its global ID.\n",
        "\n",
        "Formula: `global_id = blockIdx.x * blockDim.x + threadIdx.x`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebZn-68Y1tRR"
      },
      "outputs": [],
      "source": [
        "%%writefile hello_gpu.cu\n",
        "#include <stdio.h>\n",
        "\n",
        "// Device code (runs on GPU)\n",
        "__global__ void hello() {\n",
        "    int id = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    printf(\"Hello from GPU thread %d\\n\", id);\n",
        "}\n",
        "\n",
        "// Host code (runs on CPU)\n",
        "int main() {\n",
        "    printf(\"Launching kernel with 1 block, 8 threads\\n\\n\");\n",
        "\n",
        "    hello<<<1, 8>>>();  // 1 block, 8 threads\n",
        "\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    printf(\"\\nDone!\\n\");\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiwG-kK51tRR"
      },
      "outputs": [],
      "source": [
        "!nvcc hello_gpu.cu -o hello_gpu\n",
        "!./hello_gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHwIrJwg1tRS"
      },
      "source": [
        "**Explanation:**\n",
        "- `__global__` = function runs on GPU\n",
        "- `<<<blocks, threads>>>` = kernel launch syntax\n",
        "- `blockIdx.x` = which block (0 in our case)\n",
        "- `blockDim.x` = threads per block (8)\n",
        "- `threadIdx.x` = thread within block (0-7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icmB1irP1tRS"
      },
      "source": [
        "## Q5. Host and Device Memory\n",
        "\n",
        "Demonstrate memory management between CPU (host) and GPU (device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Rozz9TJ1tRS"
      },
      "outputs": [],
      "source": [
        "%%writefile memory_demo.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void printArray(int *arr, int n) {\n",
        "    int i = threadIdx.x;\n",
        "    if(i < n) {\n",
        "        printf(\"GPU: arr[%d] = %d\\n\", i, arr[i]);\n",
        "        arr[i] = arr[i] * 10;  // modify on GPU\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int n = 5;\n",
        "    int size = n * sizeof(int);\n",
        "\n",
        "    // Step 1: Create array on host (CPU)\n",
        "    int h_arr[5] = {10, 20, 30, 40, 50};\n",
        "\n",
        "    printf(\"Original array on CPU:\\n\");\n",
        "    for(int i=0; i<n; i++) {\n",
        "        printf(\"%d \", h_arr[i]);\n",
        "    }\n",
        "    printf(\"\\n\\n\");\n",
        "\n",
        "    // Step 2: Allocate memory on device (GPU)\n",
        "    int *d_arr;\n",
        "    cudaMalloc(&d_arr, size);\n",
        "\n",
        "    // Step 3: Copy from host to device\n",
        "    cudaMemcpy(d_arr, h_arr, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Step 4: Run kernel on GPU\n",
        "    printf(\"Running GPU kernel...\\n\");\n",
        "    printArray<<<1, n>>>(d_arr, n);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Step 5: Copy back from device to host\n",
        "    cudaMemcpy(h_arr, d_arr, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    printf(\"\\nModified array on CPU:\\n\");\n",
        "    for(int i=0; i<n; i++) {\n",
        "        printf(\"%d \", h_arr[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "\n",
        "    // Step 6: Free GPU memory\n",
        "    cudaFree(d_arr);\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCUGUbsb1tRS"
      },
      "outputs": [],
      "source": [
        "!nvcc memory_demo.cu -o memory_demo\n",
        "!./memory_demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_W03asin1tRS"
      },
      "source": [
        "**Key functions:**\n",
        "- `cudaMalloc()` - allocate memory on GPU\n",
        "- `cudaMemcpy()` - copy between CPU and GPU\n",
        "- `cudaFree()` - free GPU memory\n",
        "\n",
        "**Important:** CPU can't directly access GPU memory and vice versa!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uxM-UGy1tRS"
      },
      "source": [
        "## Q6. Compare List/Tuple vs NumPy Performance\n",
        "\n",
        "Testing which is faster for numerical operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAYWYy0J1tRS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "sizes = [10000, 100000, 1000000]\n",
        "\n",
        "print(\"Size\\t\\tList(ms)\\tTuple(ms)\\tNumPy(ms)\\tSpeedup\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for n in sizes:\n",
        "    # Test with list\n",
        "    start = time.time()\n",
        "    data = list(range(n))\n",
        "    result = sum([x*2 for x in data])\n",
        "    list_time = (time.time() - start) * 1000\n",
        "\n",
        "    # Test with tuple\n",
        "    start = time.time()\n",
        "    data = tuple(range(n))\n",
        "    result = sum([x*2 for x in data])\n",
        "    tuple_time = (time.time() - start) * 1000\n",
        "\n",
        "    # Test with numpy\n",
        "    start = time.time()\n",
        "    data = np.arange(n)\n",
        "    result = np.sum(data * 2)\n",
        "    numpy_time = (time.time() - start) * 1000\n",
        "\n",
        "    speedup = list_time / numpy_time\n",
        "\n",
        "    print(f\"{n}\\t\\t{list_time:.2f}\\t\\t{tuple_time:.2f}\\t\\t{numpy_time:.2f}\\t\\t{speedup:.1f}x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCW0K5401tRS"
      },
      "source": [
        "**Observation:**\n",
        "- NumPy is much faster (10-30x)\n",
        "- Uses optimized C code internally\n",
        "- Better for math operations\n",
        "- Lists/tuples are more flexible but slower"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5o39uqj1tRS"
      },
      "outputs": [],
      "source": [
        "# quick visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "methods = ['List', 'Tuple', 'NumPy']\n",
        "times = [150, 145, 8]  # example values\n",
        "\n",
        "plt.bar(methods, times)\n",
        "plt.ylabel('Time (ms)')\n",
        "plt.title('Performance Comparison (N=1M)')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}